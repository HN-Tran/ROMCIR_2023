<div id="table:gdataset">

|             |           | **GermEval 2021** |          |          | **GermEval 2018** | **GermEval 2019 T2** |
|:-----------:|:----------|:-----------------:|:--------:|:--------:|:-----------------:|:--------------------:|
| **Dataset** | **Label** |     **ST 1**      | **ST 2** | **ST 3** |     **ST 1**      |       **ST 1**       |
|    Train    | Positive  |       1122        |   865    |   1103   |       1688        |         1287         |
|             | Negative  |       2122        |   2379   |   2141   |       3321        |         2707         |
|    Test     | Positive  |        350        |   253    |   314    |       1202        |         970          |
|             | Negative  |        594        |   691    |   630    |       2330        |         2061         |

GermEval Dataset Sizes

</div>

<div id="table:mtranslator">

| **Translation Service** | **Run 1** | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard** | **Soft** |
|:------------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:--------:|:--------:|
| Google Translate        |   69.48   |   67.08   |   67.67   |   68.74   |   68.28   |  68.39   |  68.42   |
| DeepL Translator        |   70.01   |   70.22   |   69.24   |   68.26   |   67.67   |  70.13   |  70.09   |

Decision 2 – Machine Translation

</div>

<div id="table:shuffling">

| **Shuffling Strategy** | **Run 1** | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard** | **Soft** |
|:-----------------------|:---------:|:---------:|:---------:|:---------:|:---------:|:--------:|:--------:|
| Stratified K-Fold CV   |   68.78   |   67.79   |   68.21   |   69.62   |   67.29   |  68.99   |  69.59   |
| Random Seed            |   70.01   |   70.22   |   69.24   |   68.26   |   67.67   |  70.13   |  70.09   |

Decision 3 – Shuffling Strategy

</div>

<div id="table:preprocessing">

|            |             |           |           |           |           |           |          |          |
|:-----------|:------------|:---------:|:---------:|:---------:|:---------:|:---------:|:--------:|:--------:|
|            |             |           |           |           |           |           |          |          |
| Processing | **Dataset** | **Run 1** | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard** | **Soft** |
| With       | Dev         |   70.33   |   68.20   |   67.61   |   71.12   |   70.14   |    —     |    —     |
|            | Test        |   71.28   |   70.52   |   70.33   |   71.72   |   70.83   |  71.64   |  72.56   |
| Without    | Dev         |   48.56   |   70.43   |   68.91   |   69.81   |   70.57   |    —     |    —     |
|            | Test        |   59.61   |   72.27   |   71.65   |   72.02   |   71.91   |  73.60   |  72.24   |

Decision 4 – Pre-Processing

</div>


<div id="table:g21s1">

| **Model**                  |               **Run 1**               |               **Run 2**               |               **Run 3**               |               **Run 4**               |               **Run 5**               |               **Hard**                | **Soft**  |
|:---------------------------|:-------------------------------------:|:-------------------------------------:|:-------------------------------------:|:-------------------------------------:|:-------------------------------------:|:-------------------------------------:|:---------:|
| GBERT<sub>base</sub>       |                 66.73                 |                 68.33                 |                 69.01                 |                 69.03                 |                 67.39                 |                 68.11                 |   67.84   |
| GELECTRA<sub>base</sub>    |                 68.55                 |                 68.19                 |                 68.19                 |                 70.14                 |                 68.71                 |                 69.38                 |   69.68   |
| BERTweet<sub>base</sub>    |                 70.01                 |                 70.22                 |                 69.24                 |                 68.26                 |                 67.67                 |                 70.13                 |   70.09   |
| BERT<sub>base</sub>        |                 65.90                 |                 65.57                 |                 63.55                 |                 61.78                 |                 63.74                 |                 64.68                 |   64.71   |
| XLM-R<sub>base</sub> (de)  |                 62.16                 |                 66.57                 |                 66.15                 |                 68.14                 |                 67.29                 |                 67.37                 |   67.21   |
| XLM-R<sub>base</sub> (en)  |                 66.24                 |                 67.10                 |                 68.33                 |                 66.45                 |                 67.44                 |                 68.24                 |   68.20   |
| T5<sub>base</sub>          |                 64.89                 |                 65.74                 |                 63.62                 |                 64.11                 |                 65.53                 |                 66.32                 |     —     |
| T5v1.1                     |                 60.76                 | <span style="color: red">49.48</span> | <span style="color: red">52.01</span> | <span style="color: red">52.93</span> | <span style="color: red">52.55</span> | <span style="color: red">54.85</span> |     —     |
| mT5<sub>base</sub> (de)    |                 61.86                 |                 64.98                 |                 64.39                 |                 64.21                 |                 65.77                 |                 66.20                 |     —     |
| mT5<sub>base</sub> (en)    |                 61.25                 |                 64.20                 |                 64.35                 |                 63.64                 |                 65.82                 |                 64.06                 |     —     |
| GBERT<sub>large</sub>      |               **72.76**               |                 71.28                 |                 70.23                 |                 70.13                 |               **72.34**               |               **72.09**               | **72.69** |
| GELECTRA<sub>large</sub>   |               **72.46**               |               **72.34**               |               **71.87**               |                 69.83                 |                 70.68                 |                 71.62                 |   71.72   |
| BERTweet<sub>large</sub>   | <span style="color: red">59.61</span> |               **72.27**               |                 71.65                 |               **72.02**               |                 71.91                 |               **73.60**               | **72.24** |
| BERT<sub>large</sub>       |                 65.11                 |                 65.91                 |                 66.33                 |                 63.99                 |                 64.93                 |                 67.00                 |   65.26   |
| XLM-R<sub>large</sub> (de) |                 67.97                 |                 67.37                 |                 68.87                 | <span style="color: red">60.43</span> |                 67.99                 |                 69.04                 |   69.12   |
| XLM-R<sub>large</sub> (en) | <span style="color: red">52.48</span> |                 69.05                 |                 70.52                 |                 70.74                 |                 69.90                 |                 71.71                 |   71.48   |
| Previous SOTA              |             71.75 (FHAC)              |                                       |                                       |                                       |                                       |                                       |           |

GermEval 2021 Subtask 1 – Toxic Comment Classification

</div>

<div id="table:g21s2">

| **Model**                  |          **Run 1**           | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard**  | **Soft**  |
|:---------------------------|:----------------------------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| GBERT<sub>base</sub>       |            68.48             |   68.41   |   67.68   |   68.46   | **70.41** |   68.22   |   68.30   |
| GELECTRA<sub>base</sub>    |            68.06             |   67.92   |   67.72   |   66.87   |   67.65   |   67.96   |   67.60   |
| BERTweet<sub>base</sub>    |            68.08             |   67.47   |   67.89   |   68.19   |   66.32   |   68.23   |   68.84   |
| BERT<sub>base</sub>        |          **70.16**           |   69.24   |   68.41   |   68.10   |   68.77   |   68.89   |   69.39   |
| XLM-R<sub>base</sub> (de)  |            66.59             |   69.05   |   67.45   |   67.94   |   68.31   |   68.49   |   67.90   |
| XLM-R<sub>base</sub> (en)  |            67.79             |   68.49   |   68.65   | **70.13** |   69.17   |   69.11   | **69.72** |
| GBERT<sub>large</sub>      |          **70.21**           |   68.22   |   69.96   |   69.30   |   68.70   |   69.45   |   68.89   |
| GELECTRA<sub>large</sub>   |            69.29             | **70.36** | **70.19** | **70.70** |   69.54   | **70.16** | **70.24** |
| BERTweet<sub>large</sub>   |            69.95             |   68.66   |   69.09   | **70.70** |   68.57   |   69.82   | **70.36** |
| BERT<sub>large</sub>       |            68.16             |   68.64   |   69.18   | **70.53** |   69.86   |   69.86   |   69.47   |
| XLM-R<sub>large</sub> (de) |            69.40             | **70.28** |   66.42   |   70.03   |   69.30   |   69.51   |   68.60   |
| XLM-R<sub>large</sub> (en) |            69.98             |   69.40   |   66.55   |   68.85   |   67.34   |   68.77   | **69.99** |
| Previous SOTA              | 69.98 (Data Science Kitchen) |           |           |           |           |           |           |

GermEval 2021 Subtask 2 – Engaging Comment Classification

</div>

<div id="table:g21s3">

| **Model**                  |  **Run 1**   | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard**  | **Soft**  |
|:---------------------------|:------------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| GBERT<sub>base</sub>       |    74.55     |   73.94   |   74.16   |   72.02   |   72.93   |   74.23   |   74.78   |
| GELECTRA<sub>base</sub>    |  **76.78**   | **76.59** |   75.52   |   75.75   |   75.81   | **76.52** | **77.11** |
| BERTweet<sub>base</sub>    |    74.65     |   74.66   |   75.48   |   74.66   |   74.51   |   75.51   |   75.47   |
| BERT<sub>base</sub>        |    73.25     |   71.92   |   74.54   |   73.16   |   73.21   |   73.36   |   72.71   |
| XLM-R<sub>base</sub> (de)  |    73.04     |   73.79   |   72.88   |   72.60   |   73.81   |   73.84   |   74.26   |
| XLM-R<sub>base</sub> (en)  |    74.71     |   73.94   |   74.16   |   75.14   |   72.54   |   74.35   |   74.61   |
| GBERT<sub>large</sub>      |    74.43     |   75.70   |   76.24   |   73.42   |   74.02   |   75.77   |   76.10   |
| GELECTRA<sub>large</sub>   |    75.48     |   74.62   |   75.32   |   74.59   |   73.94   |   75.06   |   74.26   |
| BERTweet<sub>large</sub>   |    74.81     |   75.19   |   73.28   |   73.33   |   73.91   |   75.14   |   75.48   |
| BERT<sub>large</sub>       |    73.67     |   73.50   |   74.27   |   72.06   |   73.14   |   74.58   |   75.07   |
| XLM-R<sub>large</sub> (de) |    74.95     |   76.20   |   75.69   |   74.36   |   74.96   | **76.36** | **76.82** |
| XLM-R<sub>large</sub> (en) |    74.78     |   74.90   |   73.46   |   73.98   |   75.80   | **76.54** | **77.44** |
| Previous SOTA              | 76.26 (FHAC) |           |           |           |           |           |           |

GermEval 2021 Subtask 3 – Fact-Claiming Comment Classification

</div>

<div id="table:g18s1">

| **Model**                 |                    **Run 1**                     | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard**  | **Soft** |
|:--------------------------|:------------------------------------------------:|:---------:|:---------:|:---------:|:---------:|:---------:|:--------:|
| GBERT<sub>base</sub>      |                      74.66                       |   76.43   |   75.28   |   75.17   |   76.76   |   76.28   |  75.91   |
| GELECTRA<sub>base</sub>   |                      75.10                       |   74.69   |   75.45   |   75.58   |   74.35   |   75.45   |  75.37   |
| BERTweet<sub>base</sub>   |                      78.21                       |   77.35   |   77.62   |   78.28   |   77.95   |   78.02   |  78.05   |
| BERT<sub>base</sub>       |                      77.35                       |   76.58   |   76.03   |   76.60   |   76.88   |   77.23   |  77.17   |
| XLM-R<sub>base</sub> (de) |                      75.92                       |   76.32   |   75.26   |   75.42   |   75.13   |   75.71   |  76.00   |
| XLM-R<sub>base</sub> (en) |                      77.19                       |   76.07   |   76.85   |   76.50   |   76.18   |   76.67   |  77.04   |
| GBERT<sub>large</sub>     |                      80.30                       |   80.50   |   80.67   |   79.97   |   80.69   | **80.74** |  80.63   |
| GELECTRA<sub>large</sub>  |                      80.15                       |   79.49   |   80.15   |   79.46   |   79.42   |   80.06   |  79.85   |
| BERTweet<sub>large</sub>  |                      79.13                       |   79.99   |   78.92   |   79.39   |   79.93   |   79.97   |  79.86   |
| BERT<sub>large</sub>      |                      77.47                       |   78.11   |   76.43   |   77.41   |   77.84   |   78.34   |  78.32   |
| Previous SOTA             | 76.77 (TU Wien), 80.08 (GBERT), 80.70 (GELECTRA) |           |           |           |           |           |          |

GermEval 2018 Subtask 1 – Offensive Comment Classification
  
</div>

<div id="table:g19s2">

| **Model**                 |  **Run 1**  | **Run 2** | **Run 3** | **Run 4** | **Run 5** | **Hard**  | **Soft**  |
|:--------------------------|:-----------:|:---------:|:---------:|:---------:|:---------:|:---------:|:---------:|
| GBERT<sub>base</sub>      |    75.19    |   75.95   |   76.22   |   75.08   |   75.65   |   76.50   |   76.64   |
| GELECTRA<sub>base</sub>   |    74.95    |   74.58   |   75.70   |   74.99   |   73.41   |   75.15   |   74.92   |
| BERTweet<sub>base</sub>   |  **77.24**  | **77.35** |   76.65   | **77.42** | **76.97** | **77.23** | **77.44** |
| BERT<sub>base</sub>       |    75.68    |   76.35   |   75.89   |   76.26   |   75.69   |   76.63   |   76.55   |
| XLM-R<sub>base</sub> (de) |    75.32    |   74.92   |   73.85   |   74.86   |   75.05   |   75.51   |   75.17   |
| XLM-R<sub>base</sub> (en) |    76.52    |   75.62   |   76.46   | **77.04** |   76.70   | **77.35** | **77.11** |
| GBERT<sub>large</sub>     |  **79.32**  | **79.57** | **79.92** | **79.42** | **79.76** | **80.06** | **80.23** |
| GELECTRA<sub>large</sub>  |  **80.42**  | **79.98** | **81.16** | **80.11** | **79.74** | **80.80** | **80.79** |
| BERTweet<sub>large</sub>  |  **78.80**  | **78.99** | **78.60** | **79.17** | **78.90** | **79.56** | **79.86** |
| BERT<sub>large</sub>      |  **77.02**  | **77.37** |   75.75   | **77.57** |   75.81   | **77.79** | **77.79** |
| Previous SOTA             | 76.95 (UPB) |           |           |           |           |           |           |

GermEval 2019 Task 2 Subtask 1 – Offensive Comment Classification

</div>
